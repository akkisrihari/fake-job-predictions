{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr+hvfI5ewC/rPIhPBKMIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkisrihari/fake-job-predictions/blob/main/Fraudulentjobdetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ─── Step 1: Load Data ─────────────────────────────────────────────────────────\n",
        "train_url = \"https://coding-platform.s3.amazonaws.com/dev/lms/tickets/4c8465f0-fce0-484f-8497-d25feaa8e995/NqndMEyZakuimmFI.csv\"\n",
        "test_url  = \"https://coding-platform.s3.amazonaws.com/dev/lms/tickets/cab5b1bf-9132-4399-8ed5-2c049fcc89f8/0tkf3jUGLYjCEJGz.csv\"\n",
        "\n",
        "# ✅ Download the training and test datasets to the Colab environment\n",
        "!wget -O NqndMEyZakuimmFI.csv \"https://coding-platform.s3.amazonaws.com/dev/lms/tickets/4c8465f0-fce0-484f-8497-d25feaa8e995/NqndMEyZakuimmFI.csv\"\n",
        "!wget -O 0tkf3jUGLYjCEJGz.csv \"https://coding-platform.s3.amazonaws.com/dev/lms/tickets/cab5b1bf-9132-4399-8ed5-2c049fcc89f8/0tkf3jUGLYjCEJGz.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_url)\n",
        "test_df  = pd.read_csv(test_url)\n",
        "\n",
        "print(\"Training set shape:\", train_df.shape)\n",
        "print(\"Test set shape:\", test_df.shape)\n",
        "\n",
        "# ─── Step 2: Preprocessing Function ───────────────────────────────────────────\n",
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    # Combine key text columns and fill NaNs\n",
        "    cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
        "    df['text'] = df[cols].fillna('').agg(' '.join, axis=1)\n",
        "    # Clean whitespace\n",
        "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "    return df\n",
        "\n",
        "train_p = preprocess(train_df)\n",
        "test_p  = preprocess(test_df)\n",
        "\n",
        "# ─── Step 3: Vectorization & Model Training ─────────────────────────────────\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_p['text'])\n",
        "y_train = train_df['fraudulent']\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Optional: Check training F1-score\n",
        "train_preds = model.predict(X_train)\n",
        "print(\"Training F1-score:\", f1_score(y_train, train_preds))\n",
        "\n",
        "# ─── Step 4: Predict on Test Set ─────────────────────────────────────────────\n",
        "X_test = vectorizer.transform(test_p['text'])\n",
        "test_df['fraud_probability']   = model.predict_proba(X_test)[:, 1]\n",
        "test_df['predicted_fraudulent'] = model.predict(X_test)\n",
        "\n",
        "# ─── Step 5: View Top-10 Suspicious Listings ────────────────────────────────\n",
        "top10 = test_df.sort_values(by='fraud_probability', ascending=False).head(10)\n",
        "print(top10[['title', 'fraud_probability']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKL6U-Bhdp3W",
        "outputId": "1fa58aae-5aac-4b55-c41f-66d719f8d7a0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-15 14:40:07--  https://coding-platform.s3.amazonaws.com/dev/lms/tickets/4c8465f0-fce0-484f-8497-d25feaa8e995/NqndMEyZakuimmFI.csv\n",
            "Resolving coding-platform.s3.amazonaws.com (coding-platform.s3.amazonaws.com)... 3.5.210.228, 52.219.158.99, 3.5.208.8, ...\n",
            "Connecting to coding-platform.s3.amazonaws.com (coding-platform.s3.amazonaws.com)|3.5.210.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39855051 (38M) [text/csv]\n",
            "Saving to: ‘NqndMEyZakuimmFI.csv’\n",
            "\n",
            "NqndMEyZakuimmFI.cs 100%[===================>]  38.01M  8.61MB/s    in 4.4s    \n",
            "\n",
            "2025-06-15 14:40:12 (8.61 MB/s) - ‘NqndMEyZakuimmFI.csv’ saved [39855051/39855051]\n",
            "\n",
            "--2025-06-15 14:40:12--  https://coding-platform.s3.amazonaws.com/dev/lms/tickets/cab5b1bf-9132-4399-8ed5-2c049fcc89f8/0tkf3jUGLYjCEJGz.csv\n",
            "Resolving coding-platform.s3.amazonaws.com (coding-platform.s3.amazonaws.com)... 52.219.66.20, 3.5.210.34, 16.12.40.15, ...\n",
            "Connecting to coding-platform.s3.amazonaws.com (coding-platform.s3.amazonaws.com)|52.219.66.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10181711 (9.7M) [text/csv]\n",
            "Saving to: ‘0tkf3jUGLYjCEJGz.csv’\n",
            "\n",
            "0tkf3jUGLYjCEJGz.cs 100%[===================>]   9.71M  4.52MB/s    in 2.2s    \n",
            "\n",
            "2025-06-15 14:40:16 (4.52 MB/s) - ‘0tkf3jUGLYjCEJGz.csv’ saved [10181711/10181711]\n",
            "\n",
            "Training set shape: (14304, 18)\n",
            "Test set shape: (3576, 17)\n",
            "Training F1-score: 0.7854297097324986\n",
            "                                                  title  fraud_probability\n",
            "3466                 Senior Engineering Product Manager           0.997842\n",
            "2794  Principal/Senior Mechanical Engineer (Package ...           0.995369\n",
            "2930                                Senior QA Engineer            0.992249\n",
            "2204  Home Based Payroll Data Entry Clerk Position -...           0.992045\n",
            "2343  Home Based Payroll Data Entry Clerk Position -...           0.992045\n",
            "106   Payroll Data Coordinator Positions - Earn $100...           0.988977\n",
            "2751  Payroll Data Coordinator Positions - Earn $100...           0.988977\n",
            "3071  Payroll Data Coordinator Positions - Earn $100...           0.988977\n",
            "84    Payroll Data Coordinator Positions - Earn $100...           0.988977\n",
            "1190  Payroll Data Coordinator Positions - Earn $100...           0.988977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''\n",
        "# Sample Python code in app.py\n",
        "\n",
        "def greet():\n",
        "    print(\"Hello from app.py!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    greet()\n",
        "'''\n",
        "\n",
        "# Save it to a file\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"✅ app.py has been created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btr7EcsJdqcN",
        "outputId": "26b8819b-2396-41a6-bb04-acc149e71cc3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ app.py has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48QfRAmMqzZ2",
        "outputId": "d83b707c-9d3a-4353-cf8e-c4b83182a042"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c0sFXtvpczu",
        "outputId": "2cfe91c0-0d20-4311-a0a8-c9452c34b0e9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "st.set_page_config(page_title=\"Fake Job Detector\", layout=\"wide\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import shap\n",
        "from wordcloud import WordCloud\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "import joblib\n",
        "\n",
        "# Load original data\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    train_df = pd.read_csv('NqndMEyZakuimmFI.csv')\n",
        "    test_df = pd.read_csv('0tkf3jUGLYjCEJGz.csv')\n",
        "    train_df.fillna('', inplace=True)\n",
        "    test_df.fillna('', inplace=True)\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_data()\n",
        "\n",
        "# Combine text\n",
        "def combine_text(data):\n",
        "    return data['title'] + ' ' + data['company_profile'] + ' ' + data['description'] + ' ' + data['requirements'] + ' ' + data['benefits']\n",
        "\n",
        "train_df['text'] = combine_text(train_df)\n",
        "test_df['text'] = combine_text(test_df)\n",
        "\n",
        "# Vectorize & Train function\n",
        "def train_model(X_texts, y_labels):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "    X_vec = vectorizer.fit_transform(X_texts)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_vec, y_labels)\n",
        "    joblib.dump(model, 'model.pkl')\n",
        "    joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "    return model, vectorizer\n",
        "\n",
        "# Load or train model\n",
        "try:\n",
        "    model = joblib.load(\"model.pkl\")\n",
        "    vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "except:\n",
        "    model, vectorizer = train_model(train_df['text'], train_df['fraudulent'])\n",
        "\n",
        "# SHAP setup\n",
        "explainer = shap.Explainer(model, vectorizer.transform(train_df['text']), feature_names=vectorizer.get_feature_names_out())\n",
        "\n",
        "# 🔔 Email alert function\n",
        "def send_email_alert(subject, body, to_email):\n",
        "    from_email = \"your_email@gmail.com\"\n",
        "    from_password = \"your_app_password\"\n",
        "    msg = MIMEText(body)\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg[\"From\"] = from_email\n",
        "    msg[\"To\"] = to_email\n",
        "    try:\n",
        "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
        "            server.login(from_email, from_password)\n",
        "            server.sendmail(from_email, to_email, msg.as_string())\n",
        "        print(\"Email alert sent.\")\n",
        "    except Exception as e:\n",
        "        print(\"Email failed:\", e)\n",
        "\n",
        "# UI\n",
        "st.title(\"💼 Fake Job Posting Detector\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"📂 Upload a CSV file for prediction or retraining\", type=[\"csv\"])\n",
        "\n",
        "retrain = st.checkbox(\"🔁 Retrain model with uploaded data\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    input_df = pd.read_csv(uploaded_file)\n",
        "    input_df.fillna('', inplace=True)\n",
        "    input_df['text'] = combine_text(input_df)\n",
        "\n",
        "    if retrain:\n",
        "        if 'fraudulent' in input_df.columns:\n",
        "            st.success(\"✅ Retraining model...\")\n",
        "            model, vectorizer = train_model(input_df['text'], input_df['fraudulent'])\n",
        "            explainer = shap.Explainer(model, vectorizer.transform(input_df['text']), feature_names=vectorizer.get_feature_names_out())\n",
        "            st.success(\"🎉 Model retrained and saved successfully.\")\n",
        "        else:\n",
        "            st.error(\"❌ 'fraudulent' column not found for training. Cannot retrain.\")\n",
        "\n",
        "    else:\n",
        "        X_input = vectorizer.transform(input_df['text'])\n",
        "        predictions = model.predict(X_input)\n",
        "        input_df['prediction'] = predictions\n",
        "\n",
        "        st.write(\"### 🔎 Prediction Results (0 = Real, 1 = Fake):\")\n",
        "        st.dataframe(input_df[['title', 'company_profile', 'prediction']])\n",
        "\n",
        "        st.subheader(\"🧠 SHAP Explanation for First Prediction\")\n",
        "        shap_values = explainer(X_input[:1])\n",
        "        shap.plots.bar(shap_values[0], show=False)\n",
        "        st.pyplot(plt)\n",
        "\n",
        "        csv = input_df[['title', 'company_profile', 'prediction']].to_csv(index=False)\n",
        "        st.download_button(\"📥 Download Predictions\", data=csv, file_name='predictions.csv', mime='text/csv')\n",
        "\n",
        "        st.subheader(\"📊 Prediction Distribution\")\n",
        "        st.bar_chart(input_df['prediction'].value_counts())\n",
        "\n",
        "        fake_jobs = input_df[input_df['prediction'] == 1]\n",
        "        if len(fake_jobs) > 0:\n",
        "            subject = \"🚨 Alert: High-Risk Job Postings Detected\"\n",
        "            body = f\"{len(fake_jobs)} suspicious job postings were detected.\\n\\nExamples:\\n\\n\" + fake_jobs[['title', 'company_profile']].head(5).to_string()\n",
        "            send_email_alert(subject, body, \"your_email@gmail.com\")\n",
        "            st.warning(f\"🚨 {len(fake_jobs)} fake jobs detected! Email alert sent.\")\n",
        "\n",
        "# 📊 Sidebar\n",
        "st.sidebar.title(\"🔎 Dataset Explorer\")\n",
        "if st.sidebar.checkbox(\"Show Training Data\"):\n",
        "    st.write(train_df.head())\n",
        "if st.sidebar.checkbox(\"Show Test Data\"):\n",
        "    st.write(test_df.head())\n",
        "if st.sidebar.checkbox(\"Show Class Distribution\"):\n",
        "    st.bar_chart(train_df['fraudulent'].value_counts())\n",
        "if st.sidebar.checkbox(\"Show Wordcloud (Fake Jobs)\"):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\n",
        "        ' '.join(train_df[train_df['fraudulent'] == 1]['text']))\n",
        "    st.image(wordcloud.to_array(), caption=\"WordCloud of Fake Job Posts\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYFp6sQae3HA",
        "outputId": "13012088-defc-474c-c6fd-4835c9b2eefc"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-15 15:01:55.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:55.482 No runtime found, using MemoryCacheStorageManager\n",
            "2025-06-15 15:01:55.498 No runtime found, using MemoryCacheStorageManager\n",
            "2025-06-15 15:01:55.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:55.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:55.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:56.027 Thread 'Thread-19': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:56.029 Thread 'Thread-19': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:57.735 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:01:57.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.527 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-15 15:02:13.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet\n"
      ],
      "metadata": {
        "id": "AZhlDqZEeQlQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2yUKeEGVuF30pGKtmhXLfZQTm5d_3FL38DBisq4TsCSfV2G68\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YARY2IofePo",
        "outputId": "de72fb8f-f6a5-4513-ed2c-263d00db25a2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()  # This kills any previously running tunnels\n",
        "\n"
      ],
      "metadata": {
        "id": "VDwT-Oaisvtf"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app running at: {public_url}\")\n",
        "!streamlit run app.py &> /dev/null &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgYvhj2aeh5Q",
        "outputId": "0d9c93b5-ae15-4454-9f77-a2cdce15187d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app running at: NgrokTunnel: \"https://ea7f-34-73-179-53.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# Load your training dataset\n",
        "url = \"https://coding-platform.s3.amazonaws.com/dev/lms/tickets/4c8465f0-fce0-484f-8497-d25feaa8e995/NqndMEyZakuimmFI.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Fill missing values\n",
        "df.fillna('', inplace=True)\n",
        "\n",
        "# Combine text fields\n",
        "def combine_text(data):\n",
        "    return data['title'] + ' ' + data['company_profile'] + ' ' + data['description'] + ' ' + data['requirements'] + ' ' + data['benefits']\n",
        "\n",
        "df['text'] = combine_text(df)\n",
        "\n",
        "# Vectorize the text\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Labels\n",
        "y = df['fraudulent']\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the model and vectorizer to use in FastAPI\n",
        "joblib.dump(model, \"model.joblib\")\n",
        "joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
        "\n",
        "print(\"✅ Model and vectorizer saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdkDXDOL32S4",
        "outputId": "2d15db9e-12c3-43eb-c801-134f82010206"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and vectorizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "import io\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load model and vectorizer\n",
        "try:\n",
        "    model = joblib.load(\"model.joblib\")\n",
        "    vectorizer = joblib.load(\"vectorizer.joblib\")\n",
        "except FileNotFoundError:\n",
        "    raise RuntimeError(\"Model or vectorizer not found. Please train and save them as 'model.joblib' and 'vectorizer.joblib'.\")\n",
        "\n",
        "def combine_text(df):\n",
        "    return df['title'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits']\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "    content = await file.read()\n",
        "    df = pd.read_csv(io.StringIO(content.decode(\"utf-8\")))\n",
        "\n",
        "    # Handle missing values\n",
        "    df.fillna('', inplace=True)\n",
        "\n",
        "    # Combine text fields\n",
        "    df['text'] = combine_text(df)\n",
        "\n",
        "    # Vectorize\n",
        "    X_input = vectorizer.transform(df['text'])\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_input)\n",
        "    df['prediction'] = preds\n",
        "\n",
        "    # Return results\n",
        "    return df[['title', 'company_profile', 'prediction']].to_dict(orient='records')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGTn70MY15MP",
        "outputId": "6fa7eb07-9b49-436c-9566-36fa5b2f4f74"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    }
  ]
}